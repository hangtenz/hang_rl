{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 9.1 Policy Gradient on Continuous CartPole\n",
    "\n",
    "## Goal\n",
    "\n",
    "- understanding policy gradient and implement it\n",
    "- understand how each hyperparameter contributes to the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import chula_rl as rl\n",
    "from chula_rl.env.cartpolecont import ContinuousCartPoleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = ContinuousCartPoleEnv()\n",
    "    env = rl.env.wrapper.EpisodeSummary(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parallel Env (VecEnv)\n",
    "\n",
    "This kind of env will take a vector of actions, returns a vector of states. This will help stabilize training (and also speed up) greatly especially in on-policy learning.\n",
    "\n",
    "Example of 2 parallel envs (you could use any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.shape: (2, 4)\n"
     ]
    }
   ],
   "source": [
    "env = rl.env.DummyVecEnv([make_env] * 2)\n",
    "s = env.reset()\n",
    "print('s.shape:', s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00973941,  0.01420194, -0.03185375, -0.03023926], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see (2, 4) which means 2 envs of 4 features (normal to CartPole).\n",
    "\n",
    "An interesting part of the parallel env is that it will \"reset\" the underlying env automatically (when it is done). This means we can always take action, do not need to care of the underlying environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Continuous CartPole\n",
    "\n",
    "This is the same as a normal CartPole. The only difference is that the action space is \"continuous\" dictated by a single \"float\" within (-1, 1). \n",
    "\n",
    "Exmaple of taking action in a parallel env: \n",
    "\n",
    "Each action has 1 dimension, parallel action becomes 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss.shape: (2, 4)\n",
      "r.shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "ss, r, done, info = env.step(np.array([[-0.8], [1.0]]))\n",
    "print('ss.shape:', ss.shape)\n",
    "print('r.shape:', r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Vec n-step Explorer\n",
    "\n",
    "In a parallel environment setting, we also need a compatible parallel explorer. The code is straightforward to the point that we have implemented it for you already. But you are welcome to read the code. \n",
    "\n",
    "Go see `chula_rl.explorer.vec_many_step_explorer`\n",
    "\n",
    "In policy gradient, we usually use an n-step return of some kind because it is more stable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 3\n",
    "n_max_interaction = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = rl.explorer.VecManyStepExplorer(n_step, n_max_interaction, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chula_rl.explorer.vec_many_step_explorer.VecManyStepExplorer at 0x1b63359a080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.policy.base_policy import BasePolicy\n",
    "import random\n",
    "class RandomPolicy(BasePolicy):\n",
    "    def __init__(self, n_action):\n",
    "        self.n_action = n_action\n",
    "\n",
    "    def step(self, state):\n",
    "        return np.array([[random.uniform(0, 1)],[random.uniform(0, 1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RandomPolicy(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': array([[[-0.01544014, -0.00956089,  0.02332065,  0.0059557 ],\n",
       "         [-0.03308207, -0.04741054,  0.0026366 , -0.02749023]],\n",
       " \n",
       "        [[-0.01563136,  0.24221022,  0.02343976, -0.36474264],\n",
       "         [-0.03403028,  0.2759392 ,  0.0020868 , -0.511738  ]],\n",
       " \n",
       "        [[-0.01078716,  0.4577368 ,  0.01614491, -0.68105304],\n",
       "         [-0.0285115 ,  0.7742144 , -0.00814796, -1.2585356 ]]],\n",
       "       dtype=float32), 'a': array([[[0.43069724],\n",
       "         [0.55245401]],\n",
       " \n",
       "        [[0.36877491],\n",
       "         [0.85127064]],\n",
       " \n",
       "        [[0.18122867],\n",
       "         [0.1421889 ]]]), 'r': array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=float32), 'done': array([[False, False],\n",
       "        [False, False],\n",
       "        [False, False]]), 'final_s': array([[-0.00163242,  0.56359565,  0.00252385, -0.83507425],\n",
       "        [-0.01302721,  0.8575508 , -0.03331867, -1.3859316 ]],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.step(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Advantage Actor-Critic (A2C) policy + n-step TD residual advantage\n",
    "\n",
    "A2C requires two components: \n",
    "- Actor (policy)\n",
    "- Critic (value function) \n",
    "\n",
    "Both are implemented as neural nets. We leave this section to you. \n",
    "\n",
    "Your A2C should subclass `chula_rl.policy.base_policy.BasePolicy`. \n",
    "\n",
    "## Words of advice: \n",
    "\n",
    "- You code will surely contain bugs! Developing in jupyter notebook might not be a good idea. \n",
    "- There is a ton of hyperparameters, it is no easy task to find the right parameters\n",
    "- Finding the right parameters might need some analysis on how the code performs which is hard if you don't \"log\" enough\n",
    "- So, log EVERYTHING, use tensorboard to your advantage\n",
    "- For example, log the std of the policy, log the current value of the value function. These will be invaluable in debugging\n",
    "- \"‡∏ó‡∏≥‡πÑ‡∏°‡∏°‡∏±‡∏ô‡∏ä‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏¥‡∏ô ~\" is a sentence to describe this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.policy.base_policy import BasePolicy\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "class DenseNetwork(models.Model):\n",
    "    def __init__(self, output_size, hidden_sizes):\n",
    "        super(DenseNetwork, self).__init__()\n",
    "        hidden_sizes.append(output_size)\n",
    "        self.linears = [layers.Dense(i,activation='relu') for i in hidden_sizes]\n",
    "    def call(self, x):\n",
    "        for l in self.linears[:-1]:\n",
    "            x = l(x)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "        env = ContinuousCartPoleEnv()\n",
    "        env = rl.env.wrapper.EpisodeSummary(env)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(BasePolicy):\n",
    "    def __init__(self,n_env,n_step,discount):\n",
    "        self.n_env = n_env\n",
    "        self.n_step = n_step\n",
    "        self.policy = policy\n",
    "        self.discount = discount\n",
    "        self.env = rl.env.DummyVecEnv([make_env] * n_env)\n",
    "        s = self.env.reset()\n",
    "        self.pi = DenseNetwork(1,[128,64])\n",
    "        self.v = DenseNetwork(1,[128,64])\n",
    "        self.optimizer = optimizers.Adam()\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    def step(self,state):\n",
    "        # return some action\n",
    "        action = self.pi(state)\n",
    "        action = tf.keras.backend.eval(action)\n",
    "        return action\n",
    "    def value(self,state):\n",
    "        #print(\"state = \",state)\n",
    "        value = self.v(state)\n",
    "        value = tf.keras.backend.eval(value)\n",
    "        return value\n",
    "    def vanilla_loss(self, q_targets, q_expected):\n",
    "        return tf.keras.losses.mse(q_targets, q_expected)\n",
    "    \n",
    "    def learn(self):\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        q = []\n",
    "        for i in range(self.n_env):\n",
    "            q.append(0)\n",
    "        q = np.array(q,dtype=float)\n",
    "        for j in range(self.n_step):\n",
    "            v = self.value(state)\n",
    "            action = self.step(state)\n",
    "            #print(\"action = \",action)\n",
    "            next_state,reward,done,info = self.env.step(action)\n",
    "            trajectory.append({'state':state,'next_state':next_state,'reward':reward,'done':done,'info':info})\n",
    "            #print(\"reward = \",reward)\n",
    "            for k in range(self.n_env):\n",
    "                q[k] += self.discount**j * reward[k]\n",
    "            state = next_state\n",
    "        v_sn = self.value(state).reshape((-1))\n",
    "        q += self.discount**n_step * v_sn\n",
    "        with tf.GradientTape() as tape:\n",
    "            #print(np.array([state[i]]))\n",
    "            q = q.reshape((3,1))\n",
    "            v = self.value(np.array(state))\n",
    "            print(\"v.shape = \",v.shape)\n",
    "            #print(\"q.shape = \",q.shape)\n",
    "            #q = q.reshape((-1,1))\n",
    "            print(\"v = \",v)\n",
    "            #print(\"q = \",q)\n",
    "            print('v-q=',v-q)\n",
    "            #loss_v = self.loss_fn(v-q,v)\n",
    "            #loss_v = tf.keras.backend.eval(loss_v)\n",
    "            #print(\"loss_v = \",loss_v)\n",
    "            x = tf.convert_to_tensor(v-q)\n",
    "            y = tf.convert_to_tensor(v)\n",
    "            loss_v = self.vanilla_loss(x, y)\n",
    "            gradients_dl = tape.gradient(loss_v,self.v.trainable_weights)\n",
    "            #print(\"v-q = \",v-q)\n",
    "            print(\"gradients_dl = \",gradients_dl)\n",
    "            dl = np.sum((v-q)*gradients_dl)\n",
    "                \n",
    "        pi = self.step(state)\n",
    "        loss_pi = self.loss_fn(self.discount**i * (q-v),pi)\n",
    "        gradients_pi = tape.gradient(loss_pi,self.pi.trainable_weights)\n",
    "        dj += (self.discount**i * (q-v)) * gradients_pi\n",
    "        self.optimizer.apply_gradients(zip(dl, self.v.trainable_weights))\n",
    "        self.optimizer.apply_gradients(zip(dj, self.pi.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2c = A2C(3,5,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a2c.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "\n",
    "If you forgot how to run it already. Here is how: \n",
    "\n",
    "```\n",
    "while True:\n",
    "    data = exp.step(policy)\n",
    "    policy.optimize_step(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: A2C + n-step Generalized Advantage\n",
    "\n",
    "You are invited to implement the same A2C but using the generalized advantage instead. Legend has it this is a better advantage estimate! üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
