{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 9.2 Deep Deterministic Policy Gradient with Continuous CartPole\n",
    "\n",
    "## Goal\n",
    "\n",
    "- understanding DDPG and implement it\n",
    "- understand how each hyperparameter contributes to the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import chula_rl as rl\n",
    "from chula_rl.env.cartpolecont import ContinuousCartPoleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = ContinuousCartPoleEnv()\n",
    "    env = rl.env.wrapper.EpisodeSummary(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl.env.DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Vec one-step explorer\n",
    "\n",
    "In a parallel environment setting, we also need a compatible parallel explorer. The code is straightforward to the point that we have implemented it for you already. But you are welcome to read the code. \n",
    "\n",
    "Go see `chula_rl.explorer.vec_one_step_explorer`\n",
    "\n",
    "In DDPG, we use one-step explorer in same fashion as DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = rl.explorer.VecOneStepExplorer(n_step, n_max_interaction, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1 Vec one-step replay\n",
    "\n",
    "DDPG uses with replays. We have implemented a vec version of one-step replay. It is a trivial extension to the non-vec one.\n",
    "\n",
    "Go see `chula_rl.explorer.vec_one_step_uniform_replay`\n",
    "\n",
    "A replay is implemented as a \"wrapper\" to the original explorer. It acts as a middle-person to call the explorer but return samples from the replay instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = rl.explorer.VecOneStepUniformReplay(exp, n_sample, n_max_size, n_env, obs_space, act_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "DDPG requires 2 components: \n",
    "- Deterministic policy\n",
    "- Critic (Q function)\n",
    "\n",
    "Both are implemented as neural nets. We leave this section to you. \n",
    "\n",
    "Your DDPG should subclass `chula_rl.policy.base_policy.BasePolicy`. \n",
    "\n",
    "## Words of advice: \n",
    "\n",
    "- You code will surely contain bugs! Developing in jupyter notebook might not be a good idea. \n",
    "- There is a ton of hyperparameters, it is no easy task to find the right parameters\n",
    "- Finding the right parameters might need some analysis on how the code performs which is hard if you don't \"log\" enough\n",
    "- So, log EVERYTHING, use tensorboard to your advantage\n",
    "- For example, log the mean action of the policys, log the current value of the value function. These will be invaluable in debugging\n",
    "- \"ทำไมมันช่างเปราะบางเหลือเกิน ~\" is a sentence to describe this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "\n",
    "If you forgot how to run it already. Here is how: \n",
    "\n",
    "```\n",
    "while True:\n",
    "    data = exp.step(policy)\n",
    "    policy.optimize_step(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:charin]",
   "language": "python",
   "name": "conda-env-charin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
